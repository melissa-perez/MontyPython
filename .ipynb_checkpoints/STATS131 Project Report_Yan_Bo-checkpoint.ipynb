{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#            < Breast Cancer Diagnosis Report \\> \n",
    "###         ——       Predict Whether the Cancer is Benign or Malignant \n",
    "####             By Jemma Le, Annie Lee, Melissa Perez, Yan Bo Zeng "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Context and Description of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Background information on the subject and field of study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast cancer is cancer that forms in the cells of the breasts. After skin cancer, breast cancer is the most common cancer diagnosed in women in the United States. Breast cancer can occur in both men and women, but it's far more common in women.\n",
    "\n",
    "Substantial support for breast cancer awareness and research funding has helped create advances in the diagnosis and treatment of breast cancer. Breast cancer survival rates have increased, and the number of deaths associated with this disease is steadily declining, largely due to factors such as earlier detection, a new personalized approach to treatment and a better understanding of the disease.\n",
    "\n",
    "The data explores ten visually assessed\n",
    "characteristics of an FNA sample which are considered relevant to breast cancer diagnosis for diagosing the stage of breast cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Information about Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Breast Cancer Diagnostic Data Set was created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian, professors at University of Wisconsin in 1992. It was constructed on the demand of building sound statistical models to predict whether a breast tumor is benign, noncancerous, or malignant, cancerous, based on the features of cell nucleus sampled from the breast tumor. According to Dr. Wolberg, this work grew out of his desire to accurately diagnose breast smasses based solely on a Fine Needle Aspiration (FNA). He identified nine visually assessed\n",
    "characteristics of an FNA sample which he considered relevant to diagnosis. A classifier was then constructed using the multi-surface method (MSM) of pattern separation on these nine\n",
    "features that successfully diagnosed 97% of new cases. Later, the resulting dataset was saved and published for free as this Breast Cancer Diagnostic Data Set.\n",
    "\n",
    "There are 10 real-valued features stored in the data set. Each of them was computed from\n",
    "a digitized image of a Fine Needle Aspirate (FNA) of a breast mass. They describe\n",
    "characteristics of the cell nuclei present in the image:\n",
    "1. Radius – mean of distances from center to points on the perimeter\n",
    "2. Texture – standard deviation of gray-scale values\n",
    "3. Perimeter\n",
    "4. Area\n",
    "5. Smoothness – local variation in radius lengths\n",
    "6. Compactness – (perimeter ^ 2 / area) – 1\n",
    "7. Concavity – severity of concave portions of the contour\n",
    "8. Concave Points – number of concave portions of the contour\n",
    "9. Symmetry\n",
    "10. Fractal Dimension – coastline approximation – 1\n",
    "\n",
    "The data set stores the mean, standard error, and “worsts”, which represent the mean of the three\n",
    "largest values of these features computed for each image, resulting in 30 features.\n",
    "\n",
    "The collection of this data set was initiated by Dr. Wolberg in 1990 as Dr. Nick Street’s research team began to the help with the image analysis. The results of the research have been\n",
    "consolidated into a software system known as Xcyt. The data collection process was performed as follows:\n",
    "\n",
    "1. An FNA was taken from the breast mass. This material was then mounted on a microscope slide and stained to highlight the cellular nuclei. A portion of the slide in which the cells are well-differentiated was then scanned using a digital camera and a frame-grabber board.\n",
    "\n",
    "2. Researchers then isolated the individual nuclei using Xcyt. Using a mouse pointer, the user drew the approximate boundary of each nucleus. Using a computer vision approach known as “snakes”, these approximations then converged to the exact nuclear boundaries.\n",
    "\n",
    "3. Once all (or most) of the nuclei had been isolated in this fashion, the program computed values for each of ten characteristics of each nuclei, measuring size, shape and texture. The mean, standard error, and extreme values of these features were computed, resulting in a total of 30 nuclear features for each sample.\n",
    "\n",
    "4. In total, the final data set consists of 569 cases, or observations, and 32 variables including case IDs, whether the case is “Benign” or “Malignant”, and 30 nuclear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Explotary Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-18f554555689>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Libraries we will use for data analysis\n",
    "import numpy as  np # Linear algebra \n",
    "import pandas as  pd # Data processing \n",
    "import matplotlib.pyplot as plt # Data visualization library\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import learning_curve, StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import subprocess\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('cancer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data Features / Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above data information, we found\n",
    "\n",
    "1) Column __id__ is an unique identifier for each observation, which cannot be used for classificaiton.\n",
    "\n",
    "2) __Diagnosis__ is our class label, which has two levels -- M (Malignant) and B (Benign)\n",
    "\n",
    "3) __Unnamed: 32__ feature includes NaN so we will need to remove them  \n",
    "\n",
    "Therefore, for further data analysis and data modeling, we will drop these unnecessary features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop  unnecessary features: id and Unnamed: 32 columns\n",
    "cancer = cancer.drop(['id','Unnamed: 32'], axis = 1)\n",
    "\n",
    "# Romove missing values \n",
    "cancer = cancer.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x includes predictors only; excluse the response variabel\n",
    "x = cancer.drop([\"diagnosis\"],axis = 1 ) \n",
    "# y includes our labels and x includes our features\n",
    "y = cancer.diagnosis    # M or B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "x_norm = (x - x.mean()) / (x.std())\n",
    "\n",
    "# Devide the variables into 3 parts based on its function\n",
    "col_mean= x_norm.columns[0:10]\n",
    "col_se = cancer.columns[10:20]\n",
    "col_worst = cancer.columns[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Summary Statistics and the Distributional Shape of Variables in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "cancer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: area_mean feature's max value is 2501 and smoothness_mean features' max 0.16340. Therefore, we will need __normalization__ before visualization, feature selection, feature extraction or classificaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics# Summa \n",
    "bening = cancer[cancer[\"diagnosis\"] == \"B\"]\n",
    "malignant =cancer[cancer[\"diagnosis\"] == \"M\"]\n",
    "\n",
    "print(\"variance: \",bening.radius_mean.var())\n",
    "print(\"describe method: \",bening.radius_mean.describe())\n",
    "\n",
    "print(\"variance: \",malignant.radius_mean.var())\n",
    "print(\"describe method: \",malignant.radius_mean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Distributional Shape of Response Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\n",
    "B, M = y.value_counts()\n",
    "print('Number of Benign: ',B)\n",
    "print('Number of Malignant : ',M)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution plot we can see, benign has higher proportion in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Distributional Shape of Predictor Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Col_mean\n",
    "data1 = pd.concat([y,x_norm[col_mean]],axis=1)\n",
    "data1 = pd.melt(data1,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data1,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings: \n",
    "\n",
    "(1)In features like __texture_mean, perimeter_mean, area_mean__ , median of the Malignant and Benign are separated, so they potentially can be good for classification. \n",
    "\n",
    "(2) However, in features like __smoothness_mean, symmetry_mean, fractal_dimension_mean__ feature, median of the Malignant and Benign does not looks like separated, so they do not gives good information for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Col_se\n",
    "data2 = pd.concat([y,x_norm[col_se]],axis=1)\n",
    "data2 = pd.melt(data2,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data2,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "- Most variables are highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Col_worst\n",
    "data3 = pd.concat([y,x_norm[col_worst]],axis=1)\n",
    "data3 = pd.melt(data3,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data3,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "- __perimeter_worst and area_worst__ are good identifier for the stage of breast cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Unusual features or outliers present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data3)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings \n",
    "\n",
    "- While looking at box plots as we can see there are several values are outliers in each variable. \n",
    "- There values can be errors or rare events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Potential Relationships in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Relationship Distribution Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot compare 2 features\n",
    "sns.jointplot(cancer.radius_mean,cancer.area_mean,kind=\"regg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings: \n",
    "- clear linear relationship between radius mean and area mean with some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(cancer.smoothness_worst,cancer.compactness_worst,kind=\"regg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cloud does not show a clear relationship between smoothness_worst and compactness_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(cancer.loc[:,'symmetry_worst'], cancer.loc[:,'fractal_dimension_worst'], kind=\"regg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The relationship between symmetry_worst and fractal_dimension_worst is not clear.\n",
    "- A lot of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between more than 2 distribution\n",
    "sns.set(style=\"white\")\n",
    "df = cancer.loc[:,['radius_worst','perimeter_worst','area_worst']]\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_diag(sns.kdeplot, lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretations\n",
    "- Correlation map is a huge matrix that includes a lot of numbers. The range of this numbers are -1 to 1.\n",
    "- Meaning of 1 is two variable are positively correlated with each other like radius mean and area mean\n",
    "- Meaning of zero is there is no correlation between variables like radius mean and fractal dimension se\n",
    "- Meaning of -1 is two variables are negatively correlated with each other like radius mean and fractal dimension mean.\n",
    "- Actually correlation between of them is not -1, it is -0.3 but the idea is that if sign of correlation is negative that means that there is negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In feature selection, we are going to draw a correlation graph so that we can remove multicolinearity\n",
    "- Multicolinearity means the columns are dependenig on each other so we should avoid it because what is the use of using same column twice \n",
    "- We will do this analysis for features_mean first then we will do for others and will see who is doing best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cancer[col_mean].corr() \n",
    "plt.figure(figsize=(18,18))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
    "           cmap= 'copper') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings: \n",
    "\n",
    "1. the radius, parameter and area are highly correlated as expected from their relation so from these we will use anyone of them\n",
    "\n",
    "2. compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here\n",
    "\n",
    "3. so selected parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n",
    "# now these are the variables which will use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split our data into train and test\n",
    "train, test = train_test_split(cancer, test_size = 0.3, random_state=1)# in this our main data is splitted into train and test\n",
    "# we can check their dimension\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[prediction_var]# taking the training data input \n",
    "train_y=train.diagnosis# This is output of our training data\n",
    "# same we have to do for test\n",
    "test_X= test[prediction_var] # taking test data inputs\n",
    "test_y =test.diagnosis   #output value of test dat\n",
    "X = cancer[prediction_var] # Making data for CV\n",
    "y = pd.get_dummies(cancer[\"diagnosis\"]).M\n",
    "y_test_dum = pd.get_dummies(test_y).M\n",
    "y_train_dum = pd.get_dummies(train_y).M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(n_estimators=100, random_state=42)# a simple random forest model\n",
    "forest_model.fit(train_X,train_y)# now fit our model for traiing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = forest_model.predict(test_X)# predict for the test data\n",
    "# prediction will contain the predicted value by our model predicted values of dignosis column for test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, prediction))\n",
    "\n",
    "acc_RF = metrics.accuracy_score(prediction,test_y) # to check the accuracy\n",
    "# here we will use accuracy measurement between our predicted value and our test output values\n",
    "print(\"Accuracy for Random Forest: {}\".format(acc_RF)+\"\\n\")\n",
    "\n",
    "cv_prec_RF = cross_val_score(forest_model, X, y, cv=5, scoring=\"precision\")\n",
    "print(\"Precision computed using 5-fold cross-validation: {}\".format(cv_prec_RF)+\"\\n\")\n",
    "\n",
    "RFprecMean = cv_prec_RF.mean()\n",
    "print(\"Mean Precision computed using 5-fold cross-validation: {}\".format(RFprecMean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy for random forest model is 94.15 %, which seems good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model II: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1 = LogisticRegression()\n",
    "\n",
    "logit1.fit(train_X, y_train_dum)\n",
    "pred = logit1.predict(test_X)\n",
    "\n",
    "print(classification_report(y_test_dum, pred))\n",
    "\n",
    "acc_logit = metrics.accuracy_score(y_test_dum, pred)\n",
    "print(\"Accuracy for Logistic Regression: {}\".format(acc_logit)+\"\\n\")\n",
    "\n",
    "cv_prec_logit = cross_val_score(logit1, X, y, cv=5, scoring=\"precision\")\n",
    "print(\"Precision computed using 5-fold cross-validation: {}\".format(cv_prec_logit)+\"\\n\")\n",
    "\n",
    "LogitprecMean = cv_prec_logit.mean()\n",
    "print(\"Mean Precision computed using 5-fold cross-validation: {}\".format(LogitprecMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_exp = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(np.exp(logit1.coef_)))], axis = 1)\n",
    "coef_exp.columns = [\"Predictors\", \"Exp Coef\"]\n",
    "print(coef_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model III: Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "param_dist = {\"max_depth\": np.random.randint(1, 6, size=5), \"max_features\": np.random.randint(1, 6, size=5), \"min_samples_leaf\": np.random.randint(1, 6, size=5), \"criterion\": [\"gini\", \"entropy\"]}\n",
    "tree = DecisionTreeClassifier()\n",
    "treecv = RandomizedSearchCV(tree, param_dist, cv=5, random_state=42)\n",
    "\n",
    "treecv.fit(X, y)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(treecv.best_params_)+\"\\n\")\n",
    "print(\"Best score is {}\".format(treecv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(min_samples_leaf=5, max_features=5, max_depth=4, criterion=\"entropy\")\n",
    "\n",
    "tree1.fit(train_X, y_train_dum)\n",
    "pred = tree1.predict(test_X)\n",
    "\n",
    "print(classification_report(y_test_dum, pred))\n",
    "\n",
    "acc_tree = metrics.accuracy_score(y_test_dum, pred)\n",
    "print(\"Accuracy for Classification Tree: {}\".format(acc_tree)+\"\\n\")\n",
    "\n",
    "cv_prec_tree = cross_val_score(tree1, X, y, cv=5, scoring=\"precision\")\n",
    "print(\"Precision computed using 5-fold cross-validation: {}\".format(cv_prec_tree)+\"\\n\")\n",
    "\n",
    "TreeprecMean = cv_prec_tree.mean()\n",
    "print(\"Mean Precision computed using 5-fold cross-validation: {}\".format(TreeprecMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dot_data = tree.export_graphviz(tree1, out_file=None, \n",
    "                         feature_names=prediction_var,  \n",
    "                         class_names=[\"M\",\"B\"],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model IV: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is all numerical,the K-Nearest Neighbor approach seemed an appropriate choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .40, random_state=42,stratify=y)\n",
    "#Stratify the split according to the labels so that they are distributed in the training and test sets as they are in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 6 neighbors is around 90%. Setting neighbors to 6 was an arbitrary choice. The following graph shows training and testing accuracies by number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "neighbors = np.arange(1, 20)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neighbors):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "# Generate plot\n",
    "plt.title('k-NN: Varying Number of Neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the testing error is the lowest at n=3 neighbors.  Although, 90% accuracy is a great benchmark we will take a look at other quantities such as the confusion matrix and the classification report. Cross-validation will also be performed to determine the best neighbor setting. Arranging a grid from 1 to 20, the function will search to see which location provides the optimal result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "param_grid={'n_neighbors':np.arange(1,20)}\n",
    "knn=KNeighborsClassifier()\n",
    "knn_cv=GridSearchCV(knn,param_grid,cv=5)\n",
    "knn_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was 93% at 14 neighbors. We will now run KNN again to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .40, random_state=42,stratify= y)\n",
    "knn=KNeighborsClassifier(n_neighbors=19)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "(print(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we see that the confusion matrix reports only 15 Benign misclassifications and 1 Malignant misclassification. The misclassification rate is 0.07% using the KNN method with 93% precision, recall, & f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
